{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_class",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pSHTeFYvWdn",
        "outputId": "5c7be557-31c6-4ff9-f449-3cdfeb1df189"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.version.VERSION)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UujzE4EtvhQH"
      },
      "source": [
        "#Increase the precision of the presented data for better side-by-side comparison\n",
        "pd.set_option(\"display.precision\", 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKS099ZqvmY6",
        "outputId": "429bfc4e-b77f-4249-b8ef-b8dad68b28f1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4vbsf-lvoWX"
      },
      "source": [
        "data_root='/content/drive/My Drive/Colab Notebooks/train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gu-5Dglvzyw",
        "outputId": "2d90c8b9-107f-4463-c84b-8a7bae08075d"
      },
      "source": [
        "#set image shape\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "\n",
        "#set the training data directory\n",
        "TRAINING_DATA_DIR = str(data_root)\n",
        "print(TRAINING_DATA_DIR);\n",
        "\n",
        "#rescale image and split data into training and validation\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "\n",
        "#create train_generator and valid_generator \n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "TRAINING_DATA_DIR,\n",
        "subset=\"validation\",\n",
        "shuffle=True,\n",
        "target_size=IMAGE_SHAPE\n",
        ")\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "TRAINING_DATA_DIR,\n",
        "subset=\"training\",\n",
        "shuffle=True,\n",
        "target_size=IMAGE_SHAPE)\n",
        "\n",
        "#output - first line is validation data, second line is training data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/train\n",
            "Found 1249 images belonging to 10 classes.\n",
            "Found 5018 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVQwM7qswADU",
        "outputId": "32fd8851-0b70-4223-caa0-57d74e0647a1"
      },
      "source": [
        "#Generating batches of tensor image data\n",
        "image_batch_train, label_batch_train = next(iter(train_generator))\n",
        "print(\"Image batch shape: \", image_batch_train.shape)\n",
        "print(\"Label batch shape: \", label_batch_train.shape)\n",
        "dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\n",
        "print(dataset_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image batch shape:  (32, 224, 224, 3)\n",
            "Label batch shape:  (32, 10)\n",
            "['Asiatic Rice Borer' 'Brown Plant Hopper' 'Rice Gall Midge'\n",
            " 'Rice Leaf Roller' 'Rice Leafhopper' 'Rice Stemfly' 'Rice Water Weevil'\n",
            " 'Small Brown Plant Hopper' 'White Backed Plant Hopper'\n",
            " 'Yellow Rice Borer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEpJotMpwMaB",
        "outputId": "026b4c6d-60a2-478b-d7af-9b36419f7cf6"
      },
      "source": [
        "#As a base model for transfer learning, weâ€™ll use MobileNet v2 model stored on TensorFlow Hub. \n",
        "#This model has advantages to be able to work on Mobile applications. \n",
        "#For more information: https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\n",
        "model = tf.keras.Sequential([\n",
        "hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\",\n",
        "output_shape=[1280],\n",
        "trainable=False),\n",
        "tf.keras.layers.Dropout(0.4),\n",
        "tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "model.build([None, 224, 224, 3])\n",
        "model.summary()\n",
        "model.compile(\n",
        "optimizer=tf.keras.optimizers.Adam(),\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 1280)              2257984   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                12810     \n",
            "=================================================================\n",
            "Total params: 2,270,794\n",
            "Trainable params: 12,810\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93w9xz-9wrvp"
      },
      "source": [
        "Training our model. You can modify epoches and test the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idUXBbKiwY49",
        "outputId": "ae4e5934-a5da-4ee9-9f2c-9d22af016343"
      },
      "source": [
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\n",
        "hist = model.fit(\n",
        "train_generator,\n",
        "epochs=10,\n",
        "verbose=1,\n",
        "steps_per_epoch=steps_per_epoch,\n",
        "validation_data=valid_generator,\n",
        "validation_steps=val_steps_per_epoch).history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 64/157 [===========>..................] - ETA: 51:34 - loss: 2.2576 - acc: 0.2559"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSSHEDv-LsdE"
      },
      "source": [
        "#check model\n",
        "final_loss, final_accuracy = model.evaluate(valid_generator, steps = val_steps_per_epoch)\n",
        "print(\"Final loss: {:.2f}\".format(final_loss))\n",
        "print(\"Final accuracy: {:.2f}%\".format(final_accuracy * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktRXya1KLsbJ"
      },
      "source": [
        "#check plot\n",
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,50])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"acc\"])\n",
        "plt.plot(hist[\"val_acc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKS35GG-wwud"
      },
      "source": [
        "#Exporting model after training\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_mode.h5') \n",
        "insect_model = tf.keras.models.load_model(('saved_model/my_mode.h5'),custom_objects={'KerasLayer':hub.KerasLayer})\n",
        "insect_model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6m1zD-fBaR0"
      },
      "source": [
        "!ls saved_model\n",
        "!ls saved_model/my_mode.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkR3Xkfpw6KA"
      },
      "source": [
        "#Get images and labels batch from validation dataset generator\n",
        "val_image_batch, val_label_batch = next(iter(valid_generator))\n",
        "true_label_ids = np.argmax(val_label_batch, axis=-1)\n",
        "print(\"Validation batch shape:\", val_image_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StUTgM4Lw99V"
      },
      "source": [
        "#Testing our Model\n",
        "tf_model_predictions = insect_model.predict(val_image_batch)\n",
        "tf_pred_dataframe = pd.DataFrame(tf_model_predictions)\n",
        "tf_pred_dataframe.columns = dataset_labels\n",
        "print(\"Prediction results for the first elements\")\n",
        "tf_pred_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHa-QcJwxEtV"
      },
      "source": [
        "#Print images batch and labels predictions\n",
        "predicted_ids = np.argmax(tf_model_predictions, axis=-1)\n",
        "predicted_labels = dataset_labels[predicted_ids]\n",
        "plt.figure(figsize=(10,9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "for n in range(30):\n",
        "  plt.subplot(6,5,n+1)\n",
        "  plt.imshow(val_image_batch[n])\n",
        "  color = \"green\" if predicted_ids[n] == true_label_ids[n] else \"red\"\n",
        "  plt.title(predicted_labels[n].title(), color=color)\n",
        "  plt.axis('off')\n",
        "  _ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56seUZL9GyCr"
      },
      "source": [
        "#Convert the model to TFLite\n",
        "!mkdir \"tflite_models\"\n",
        "TFLITE_MODEL = \"tflite_models/insect.tflite\"\n",
        "TFLITE_QUANT_MODEL = \"tflite_models/insect_quant.tflite\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka75PDX_G8WO"
      },
      "source": [
        "# Get the concrete function from the Keras model.\n",
        "run_model = tf.function(lambda x : insect_model(x))\n",
        "# Save the concrete function.\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype)\n",
        ")\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converted_tflite_model = converter.convert()\n",
        "open(TFLITE_MODEL, \"wb\").write(converted_tflite_model)\n",
        "# Convert the model to quantized version with post-training quantization\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_quant_model = converter.convert()\n",
        "open(TFLITE_QUANT_MODEL, \"wb\").write(tflite_quant_model)\n",
        "print(\"TFLite models and their sizes:\")\n",
        "!ls \"tflite_models\" -lh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ39LBSgHwCr"
      },
      "source": [
        "!ls /content/saved_model/my_mode.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoBmFmXlHVhj"
      },
      "source": [
        "# Load TFLite model and see some details about input/output\n",
        "\n",
        "tflite_interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL)\n",
        "\n",
        "input_details = tflite_interpreter.get_input_details()\n",
        "output_details = tflite_interpreter.get_output_details()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", input_details[0]['name'])\n",
        "print(\"shape:\", input_details[0]['shape'])\n",
        "print(\"type:\", input_details[0]['dtype'])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", output_details[0]['name'])\n",
        "print(\"shape:\", output_details[0]['shape'])\n",
        "print(\"type:\", output_details[0]['dtype'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHK-rEAlsfeM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKd2v0ldRess"
      },
      "source": [
        "#### Resize input and output tensors shapes\n",
        "\n",
        "Input shape of loaded TFLite model is 1x224x224x3, what means that we can make predictions for single image.\n",
        "\n",
        "Let's resize input and output tensors, so we can make predictions for batch of 32 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgVoGaW2sfUo"
      },
      "source": [
        "tflite_interpreter.resize_tensor_input(input_details[0]['index'], (32, 224, 224, 3))\n",
        "tflite_interpreter.resize_tensor_input(output_details[0]['index'], (32, 5))\n",
        "tflite_interpreter.allocate_tensors()\n",
        "\n",
        "input_details = tflite_interpreter.get_input_details()\n",
        "output_details = tflite_interpreter.get_output_details()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", input_details[0]['name'])\n",
        "print(\"shape:\", input_details[0]['shape'])\n",
        "print(\"type:\", input_details[0]['dtype'])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", output_details[0]['name'])\n",
        "print(\"shape:\", output_details[0]['shape'])\n",
        "print(\"type:\", output_details[0]['dtype'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoXy7AZxsj9q"
      },
      "source": [
        "tflite_interpreter.set_tensor(input_details[0]['index'], val_image_batch)\n",
        "\n",
        "tflite_interpreter.invoke()\n",
        "\n",
        "tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Prediction results shape:\", tflite_model_predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDq3cPoNslsa"
      },
      "source": [
        "# Convert prediction results to Pandas dataframe, for better visualization\n",
        "\n",
        "tflite_pred_dataframe = pd.DataFrame(tflite_model_predictions)\n",
        "tflite_pred_dataframe.columns = dataset_labels\n",
        "\n",
        "print(\"TFLite prediction results for the first elements\")\n",
        "tflite_pred_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "463Ku230snwN"
      },
      "source": [
        "Now let's do the same for TFLite quantized model:\n",
        "\n",
        "Load model,\n",
        "Reshape input to handle batch of images,\n",
        "Run prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G5Gmoz9spMr"
      },
      "source": [
        "# Load quantized TFLite model\n",
        "tflite_interpreter_quant = tf.lite.Interpreter(model_path=TFLITE_QUANT_MODEL)\n",
        "\n",
        "# Learn about its input and output details\n",
        "input_details = tflite_interpreter_quant.get_input_details()\n",
        "output_details = tflite_interpreter_quant.get_output_details()\n",
        "\n",
        "# Resize input and output tensors to handle batch of 32 images\n",
        "tflite_interpreter_quant.resize_tensor_input(input_details[0]['index'], (32, 224, 224, 3))\n",
        "tflite_interpreter_quant.resize_tensor_input(output_details[0]['index'], (32, 5))\n",
        "tflite_interpreter_quant.allocate_tensors()\n",
        "\n",
        "input_details = tflite_interpreter_quant.get_input_details()\n",
        "output_details = tflite_interpreter_quant.get_output_details()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", input_details[0]['name'])\n",
        "print(\"shape:\", input_details[0]['shape'])\n",
        "print(\"type:\", input_details[0]['dtype'])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", output_details[0]['name'])\n",
        "print(\"shape:\", output_details[0]['shape'])\n",
        "print(\"type:\", output_details[0]['dtype'])\n",
        "\n",
        "# Run inference\n",
        "tflite_interpreter_quant.set_tensor(input_details[0]['index'], val_image_batch)\n",
        "\n",
        "tflite_interpreter_quant.invoke()\n",
        "\n",
        "tflite_q_model_predictions = tflite_interpreter_quant.get_tensor(output_details[0]['index'])\n",
        "print(\"\\nPrediction results shape:\", tflite_q_model_predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiDKdQ-Gsq2p"
      },
      "source": [
        "# Convert prediction results to Pandas dataframe, for better visualization\n",
        "\n",
        "tflite_q_pred_dataframe = pd.DataFrame(tflite_q_model_predictions)\n",
        "tflite_q_pred_dataframe.columns = dataset_labels\n",
        "\n",
        "print(\"Quantized TFLite model prediction results for the first elements\")\n",
        "tflite_q_pred_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuu7S1aGGF_L"
      },
      "source": [
        "## Compare prediction results\n",
        "\n",
        "Now we will use Pandas to visualize results from all 3 models and find differences between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ2XJ9A6sxGX"
      },
      "source": [
        "# Concatenate results from all models\n",
        "\n",
        "all_models_dataframe = pd.concat([tf_pred_dataframe, \n",
        "                                  tflite_pred_dataframe, \n",
        "                                  tflite_q_pred_dataframe], \n",
        "                                 keys=['TF Model', 'TFLite', 'TFLite quantized'],\n",
        "                                 axis='columns')\n",
        "all_models_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bHAI-p8sy0r"
      },
      "source": [
        "# Swap columns to hava side by side comparison\n",
        "\n",
        "all_models_dataframe = all_models_dataframe.swaplevel(axis='columns')[tflite_pred_dataframe.columns]\n",
        "all_models_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq8hsxW-s0Ch"
      },
      "source": [
        "# Highlight TFLite models predictions that are different from original model\n",
        "\n",
        "def highlight_diff(data, color='yellow'):\n",
        "    attr = 'background-color: {}'.format(color)\n",
        "    other = data.xs('TF Model', axis='columns', level=-1)\n",
        "    return pd.DataFrame(np.where(data.ne(other, level=0), attr, ''),\n",
        "                        index=data.index, columns=data.columns)\n",
        "\n",
        "all_models_dataframe.style.apply(highlight_diff, axis=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkTsJ3Q-s5eH"
      },
      "source": [
        "As we can see, in most cases predictions are different between all models, usually by small factors. High-confidence predictions between TensorFlow and TensorFlow Lite models are very close to each other (in some cases there are even similar).\n",
        "Quantized model outstands the most, but this is the cost of optimizations (model weights 3-4 times less)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JthpucOxs9U0"
      },
      "source": [
        "To make prediction results even more readable, let's simplify dataframes, to show only the highest-score prediction and the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1VF39ENoaab"
      },
      "source": [
        "# Concatenation of argmax and max value for each row\n",
        "def max_values_only(data):\n",
        "  argmax_col = np.argmax(data, axis=1).reshape(-1, 1)\n",
        "  max_col = np.max(data, axis=1).reshape(-1, 1)\n",
        "  return np.concatenate([argmax_col, max_col], axis=1)\n",
        "\n",
        "# Build simplified prediction tables\n",
        "tf_model_pred_simplified = max_values_only(tf_model_predictions)\n",
        "tflite_model_pred_simplified = max_values_only(tflite_model_predictions)\n",
        "tflite_q_model_pred_simplified = max_values_only(tflite_q_model_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC3mZQCbpnyF"
      },
      "source": [
        "# Build DataFrames and present example\n",
        "columns_names = [\"Label_id\", \"Confidence\"]\n",
        "tf_model_simple_dataframe = pd.DataFrame(tf_model_pred_simplified)\n",
        "tf_model_simple_dataframe.columns = columns_names\n",
        "\n",
        "tflite_model_simple_dataframe = pd.DataFrame(tflite_model_pred_simplified)\n",
        "tflite_model_simple_dataframe.columns = columns_names\n",
        "\n",
        "tflite_q_model_simple_dataframe = pd.DataFrame(tflite_q_model_pred_simplified)\n",
        "tflite_q_model_simple_dataframe.columns = columns_names\n",
        "\n",
        "tf_model_simple_dataframe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di0vIUMN_T0I"
      },
      "source": [
        "# Concatenate results from all models\n",
        "all_models_simple_dataframe = pd.concat([tf_model_simple_dataframe, \n",
        "                                         tflite_model_simple_dataframe, \n",
        "                                         tflite_q_model_simple_dataframe], \n",
        "                                        keys=['TF Model', 'TFLite', 'TFLite quantized'],\n",
        "                                        axis='columns')\n",
        "\n",
        "# Swap columns for side-by-side comparison\n",
        "all_models_simple_dataframe = all_models_simple_dataframe.swaplevel(axis='columns')[tf_model_simple_dataframe.columns]\n",
        "\n",
        "# Highlight differences\n",
        "all_models_simple_dataframe.style.apply(highlight_diff, axis=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR8eovwEtI0L"
      },
      "source": [
        "Visualize predictions from TFLite models\n",
        "At the end let's visualize predictions from TensorFlow Lite and quantized TensorFlow Lite models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqqvh-QvdB6g"
      },
      "source": [
        "# Print images batch and labels predictions for TFLite Model\n",
        "\n",
        "tflite_predicted_ids = np.argmax(tflite_model_predictions, axis=-1)\n",
        "tflite_predicted_labels = dataset_labels[tflite_predicted_ids]\n",
        "tflite_label_id = np.argmax(val_label_batch, axis=-1)\n",
        "\n",
        "plt.figure(figsize=(10,9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "for n in range(30):\n",
        "  plt.subplot(6,5,n+1)\n",
        "  plt.imshow(val_image_batch[n])\n",
        "  color = \"green\" if tflite_predicted_ids[n] == true_label_ids[n] else \"red\"\n",
        "  plt.title(tflite_predicted_labels[n].title(), color=color)\n",
        "  plt.axis('off')\n",
        "_ = plt.suptitle(\"TFLite model predictions (green: correct, red: incorrect)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfQhjVbwo86M"
      },
      "source": [
        "# Print images batch and labels predictions for TFLite Model\n",
        "\n",
        "tflite_q_predicted_ids = np.argmax(tflite_q_model_predictions, axis=-1)\n",
        "tflite_q_predicted_labels = dataset_labels[tflite_q_predicted_ids]\n",
        "tflite_q_label_id = np.argmax(val_label_batch, axis=-1)\n",
        "\n",
        "plt.figure(figsize=(10,9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "for n in range(30):\n",
        "  plt.subplot(6,5,n+1)\n",
        "  plt.imshow(val_image_batch[n])\n",
        "  color = \"green\" if tflite_q_predicted_ids[n] == true_label_ids[n] else \"red\"\n",
        "  plt.title(tflite_q_predicted_labels[n].title(), color=color)\n",
        "  plt.axis('off')\n",
        "_ = plt.suptitle(\"Quantized TFLite model predictions (green: correct, red: incorrect)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5po0e7gAw_tn"
      },
      "source": [
        "## Export image validation batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QabY81a16vp_"
      },
      "source": [
        "Export validation batch so it can be tested client side. Below we create compressed file containing all images named with the convention:\n",
        "\n",
        "`n{}_true{}_pred{}.jpg`\n",
        "\n",
        "where the first number is index, the second - true label index, the third - value predicted by TFLite moder generated in this notebook. Example file will look similar to this: `n0_true1_pred1.jpg`.\n",
        "\n",
        "All images then will be put into client side testing code (res/assets in Android tests). Integration tests will run inference process on each image and then compare results with the ones saved in file names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDbZ3kHmtZa8"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmr4lQD5taoa"
      },
      "source": [
        "VAL_BATCH_DIR = \"validation_batch\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQsM0UjItbqy"
      },
      "source": [
        "!mkdir {VAL_BATCH_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhWe3RrhxFZj"
      },
      "source": [
        "# Export batch to *.jpg files with specific naming convention.\n",
        "# Make sure they are exported in the full quality, otherwise the inference\n",
        "# process will return different results. \n",
        "\n",
        "for n in range(32):\n",
        "  filename = \"n{:0.0f}_true{:0.0f}_pred{:0.0f}.jpg\".format(\n",
        "      n,\n",
        "      true_label_ids[n],\n",
        "      tflite_model_pred_simplified[n][0]\n",
        "  )\n",
        "  img_arr = np.copy(val_image_batch[n])\n",
        "  img_arr *= 255\n",
        "  img_arr = img_arr.astype(\"uint8\")\n",
        "  img11 = Image.fromarray(img_arr, 'RGB')\n",
        "  img11.save(\"{}/{}\".format(VAL_BATCH_DIR, filename), \"JPEG\", quality=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxsY9jwkxakd"
      },
      "source": [
        "!tar -zcvf {VAL_BATCH_DIR}.tar.gz {VAL_BATCH_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGW99IkBthL2"
      },
      "source": [
        "File validation_batch.tar.gz is ready to be downloaded, unpacked and put into client-side testing code."
      ]
    }
  ]
}